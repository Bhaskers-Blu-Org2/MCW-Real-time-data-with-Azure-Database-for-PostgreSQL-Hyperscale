![](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/master/Media/ms-cloud-workshop.png 'Microsoft Cloud Workshops')

<div class="MCWHeader1">
Managed open source databases on Azure
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
June 2019
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

Â© 2019 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at <https://www.microsoft.com/en-us/legal/intellectualproperty/Trademarks/Usage/General.aspx> are trademarks of the Microsoft group of companies. All other trademarks are property of their respective owners.

**Contents**

<!-- TOC -->

- [Trainer information](#Trainer-information)
  - [Role of the trainer](#Role-of-the-trainer)
  - [Whiteboard design session flow](#Whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#Before-the-whiteboard-design-session-How-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#During-the-whiteboard-design-session-Tips-for-an-effective-whiteboard-design-session)
- [Managed open source databases on Azure whiteboard design session student guide](#Managed-open-source-databases-on-Azure-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#Abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#Step-1-Review-the-customer-case-study)
    - [Customer situation](#Customer-situation)
    - [Customer needs](#Customer-needs)
    - [Customer objections](#Customer-objections)
    - [Infographic for common scenarios](#Infographic-for-common-scenarios)
  - [Step 2: Design a proof of concept solution](#Step-2-Design-a-proof-of-concept-solution)
  - [Step 3: Present the solution](#Step-3-Present-the-solution)
  - [Wrap-up](#Wrap-up)
  - [Additional references](#Additional-references)
- [Managed open source databases on Azure whiteboard design session trainer guide](#Managed-open-source-databases-on-Azure-whiteboard-design-session-trainer-guide)
  - [Step 1: Review the customer case study](#Step-1-Review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#Step-2-Design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#Step-3-Present-the-solution-1)
  - [Wrap-up](#Wrap-up-1)
  - [Preferred target audience](#Preferred-target-audience)
  - [Preferred solution](#Preferred-solution)
  - [Checklist of preferred objection handling](#Checklist-of-preferred-objection-handling)
  - [Customer quote (to be read back to the attendees at the end)](#Customer-quote-to-be-read-back-to-the-attendees-at-the-end)

<!-- /TOC -->

# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements

- Current customer infrastructure and architecture

- Potential issues, objectives and blockers

**Step 2: Design a proof of concept solution (60 minutes)**

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

**Outcome**

Present solution to your customer:

- Present solution

- Respond to customer objections

- Receive feedback

**Wrap-up (15 minutes)**

- Review preferred solution

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Prior to the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

**\*Have fun**! Encourage participants to have fun and share!\*

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Managed open source databases on Azure whiteboard design session student guide

## Abstract and learning objectives

\[insert what is trying to be solved for by using this workshop. . . \]

## Step 1: Review the customer case study

**Outcome**

Analyze your customer's needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1.  Meet your table participants and trainer.

2.  Read all of the directions for steps 1-3 in the student guide.

3.  As a table team, review the following customer case study.

### Customer situation

Wide World Importers (WWI) is a traditional brick and mortar business with a long track record of success, generating profits through strong retail store sales of their unique offering of affordable products from around the world. They have a great training program for new employees, that focuses on connecting with their customers and providing great face-to-face customer service. This strong focus on customer relationships has helped set WWI apart from their competitors.

Over time, WWI modernized their business by expanding to online storefronts. During this expansion period, WWI experimented with various marketing tactics to drive online sales, from offering in-store shoppers special discounts online with promotional coupons after making a purchase, to running ad campaigns targeted toward customers based on demographics and shopping habits. These marketing campaigns proved successful, prompting WWI to invest more resources to these efforts and grow their marketing team.

Today, WWI has a host of online stores for various product offerings, from their traditional product catalogs offered by their physical storefronts, to specialized categories like automotive and consumer technology products. This expansion has made it more challenging to analyze user clickstream data, online ad performance, and other marketing campaigns at scale, and to provide insights to the marketing team in real-time.

Real-time marketing analysis is provided through interactive reports and dashboards on WWI's home-grown web platform, ReMarketable. This platform has served them well, but they are currently hindered by their inability to keep up with demand. ReMarketable's primary users are members of the marketing team, and the secondary users are shoppers on their various online platforms for whom website interaction behavior is being tracked. Other sources of data are fed from online ad data generated by ads run on social media platforms and email marketing campaigns. They use this type of data to evaluate ad effectiveness and customer reach, ultimately leading to sales conversions.

Their current challenges with ReMarketable are:

1. **Scale** - WWI is using a PostgreSQL database to store ReMarketable's data. Historical data is growing by over 2.9 GB rows of data per month. It is taking consistently longer to run complex queries. Queries that used to run in 3-5 seconds now take several minutes to complete. This is impacting their users' ability to evaluate up-to-date marketing and website use statistics. Instead of providing real-time reports for all users, they have to keep delaying report runs. They have scaled up their database, but this is becoming very expensive and they will soon hit a ceiling.
2. **Multi-tenancy** - The nature of their marketing and site usage data would benefit from multi-tenancy. Some storefronts generate considerably more data than others and have more marketing analysts that run reports on them than others. WWI believes sharding their database would help take the pressure off lower-volume data stores and also help them scale out. However, this will require re-engineering their database schema and client applications. In addition, sharding will require additional maintenance and increased complexity of aggregated views. These additional challenges and required resources are why they have not pursued this option yet.
3. **Process data while generating roll-ups** - Another byproduct of outgrowing their database is that WWI is having difficulty efficiently processing and ingesting streaming data, while at the same time generating pre-aggregated data for their dashboards. The Postgres engine is well-suited to handle multiple workloads simultaneously when the databases are properly configured and you are able to appropriately scale up or scale out to multiple nodes. WWI does needs help optimizing their database to handle these demanding workloads at scale. They have looked moving to a non-relational database to speed up queries, but that option added too much complexity to manage multiple databases, losing the ability to wrap their operations inside of transactions, re-architect their application, and migrate their historical data. In addition, they rely on Postgres' ability to create complicated ways of representing and indexing their data, which is impossible to do with a column store. Their need for high transaction volume and a real-time data set ruled out a lot of off-the-shelf data warehouses, where they would need to create a lambda architecture to handle both speeds of feeds.
4. **Resilient stream processing** - WWI is processing their streaming data through a web-based cluster that balances HTTP requests in round-robin fashion. When a node is processing the data and writing it to Postgres, subsequent requests are handled by available nodes. However, if processing fails for any reason, they risk losing that data and have no way to pick up where it left off. They have tried creating their own poison queue to reprocess these failed messages, but if the failed node is unable to add the data to the queue, then it is lost. The WWI technical team is aware of existing products and services that can help improve their stream processing and add resiliency, but they currently lack the skills and bandwidth to implement a solution for these complex scenarios. They are interested to see how Azure can help them rapidly create a solution for resilient stream processing and reduce their technical debt.
5. **Advanced dashboards** - WWI creates canned reports that are displayed on their ReMarketable website. However, their developers spend a lot of time creating new reports, owing to advanced charts, graphs, and other visualizations that are usually included. They would like a way to more rapidly create reports and be able to display them on a dashboard that can be customized and show real-time updates.

### Customer needs

1. Scale our marketing PostgreSQL database to handle high data growth while reducing the amount of time to run complex queries.

2. Shard our database so we can scale out, based on our tenants and their load requirements. We need a way to do this that will reduce schema changes, maintenance, and complexity of aggregated views.

3. Need a way to efficiently ingest and process streaming data, while at the same time generating pre-aggregated data for our dashboards.

4. During our stream processing, we sometimes encounter errors and have a difficult time recovering and continuing where we left off. We need a more resilient stream processing solution to reduce errors and prevent lost data.

5. Would like a simple way to create powerful reports with a variety of visualizations. Ideally, this is something our analysts should be able to do against our live data sets.

### Customer objections

1. Does Azure offer a managed PostgreSQL database that can handle our scale requirements?

2. We are worried about the re-engineering effort involved in sharding our database, from modifying the schema to updating our applications to account for the changes.

3. Is there a way to migrate to PostgreSQL on Azure with minimal downtime?

4. We've looked at several PostgreSQL-based data platforms for adding enhancements like distributed data and scalability, but we are concerned about our existing applications being compatible and having access to the latest versions of PostgreSQL.

### Infographic for common scenarios

![Infographic for common scenarios that you can use for inspiration.](media/common-scenarios.png 'Infographic for common scenarios')

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With all participants at your table, answer the following questions and list the answers on a flip chart:

1.  Who should you present this solution to? Who is your target customer audience? Who are the decision makers?

2.  What customer business needs do you need to address with your solution?

**Design**

Directions: With all participants at your table, respond to the following questions on a flip chart:

_High-level architecture_

1. Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for creating a real-time data processing pipeline that can ingest, process, and write streaming data to a highly scalable managed PostgreSQL database on Azure. The stream processor needs to be resilient by keeping track of where it left off and prevent lost data. Include a solution for creating advanced visualizations that can be shared or embedded in external websites and mobile devices.

_Scale_

1. How will you configure the managed PostgreSQL database so it can be scaled to meet demand. Think about options for scaling up and for scaling out.

2. WWI wants to be able to scale their database out, but they've already expressed concerns about how sharding tables to accomplish this adds a lot of complexity and maintenance overhead, as well as required code changes. How would you propose they shard tables that need to have data distributed amongst the nodes?

3. How can the clickstream time series event data be partitioned into 5-minute increments to avoid creating large indexes?

_Multi-tenancy_

1. The clickstream event data is multi-tenant by nature. Each tenant is denoted by a Tenant ID which is related to the source of the clickstream feed. How can WWI shard the raw event table by tenant across multiple nodes in a way that causes little impact to existing applications and maintenance overhead?

2. WWI has expressed a desire to create rollup tables that contain pre-aggregated data for efficient reporting. These rollup tables should also be sharded by Tenant ID. How can WWI shard these tables as well and what data type can be used to rapidly obtain distinct counts within a small margin of error in a highly scalable way across partitions?

_Process data while generating roll-ups_

1. Rollup tables enable faster queries for reporting and exploration, but oftentimes require compute-heavy work to periodically run in the background to populate these tables. How would you schedule these aggregates to run on a periodic basis?

2. Within your rollup functions that perform the background aggregations, how would you implement incremental aggregations to handle late, incoming, data while keeping track of which time periods have been aggregated already?

3. Incremental aggregations sacrifice the ability to handle all types of aggregates for ease of use when tracking what has already been aggregated when processing late data. What advanced aggregation options can you use to provide highly accurate approximation in this situation?

_Resilient stream processing_

1. WWI is currently using a Kafka cluster to ingest a high volume of streaming data from various clickstream sources across their tenants. Is there a managed option in Azure that supports Kafka?

2. WWI sometimes encounters errors while stream processing, and has a difficult time recovering and continuing where they left off if the stream has to stop for any reason. What would you recommend for a resilient stream processing solution to reduce errors and prevent lost data?

3. Could your chosen stream processing solution also provide the ability to conduct batch processing against large amounts of data while sharing much of the data processing and cleansing code, as well as code to write data to PostgreSQL?

_Advanced dashboards_

1. What would you recommend Wide World Importers use to create reports and dashboards with advanced visualizations, that can be created with an intuitive visual interface, easily shared with others, embedded in external websites or mobile devices?

2. How can WWI refresh the report's data on a regular basis and provide redundancy in their synchronization process? Does this synchronization process require any inbound ports to be opened up on the computer or servers on which it runs?

**Prepare**

Directions: With all participants at your table:

1.  Identify any customer needs that are not addressed with the proposed solution.

2.  Identify the benefits of your solution.

3.  Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions:

1.  Pair with another table.

2.  One table is the Microsoft team and the other table is the customer.

3.  The Microsoft team presents their proposed solution to the customer.

4.  The customer makes one of the objections from the list of objections.

5.  The Microsoft team responds to the objection.

6.  The customer team gives feedback to the Microsoft team.

7.  Tables switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

Directions: Tables reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

|                                                         |                                                                                                    |
| ------------------------------------------------------- | :------------------------------------------------------------------------------------------------: |
| **Description**                                         |                                             **Links**                                              |
| Azure PostgreSQL documentation                          |                        <https://docs.microsoft.com/en-us/azure/postgresql/>                        |
| PostgreSQL time series data processing                  |  <https://docs.microsoft.com/en-us/azure/postgresql/tutorial-design-database-hyperscale-realtime>  |
| PostgreSQL distribution columns and multi-tenant apps   | <https://docs.microsoft.com/en-us/azure/postgresql/concepts-hyperscale-choose-distribution-column> |
| PostgreSQL HyperLogLog extension                        |                           <https://github.com/citusdata/postgresql-hll>                            |
| Azure Databricks documentation                          |         <https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks>         |
| Azure Databricks Structured Streaming                   |          <https://docs.azuredatabricks.net/spark/latest/structured-streaming/index.html>           |
| Event Hubs for Apache Kafka                             |    <https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview>     |
| Power BI documentation                                  |                            <https://docs.microsoft.com/en-us/power-bi/>                            |
| High availability clusters for On-premises data gateway |       <https://docs.microsoft.com/en-us/power-bi/service-gateway-high-availability-clusters>       |

# Managed open source databases on Azure whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your table participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let the table participants begin.

## Step 2: Design a proof of concept solution

- Check in with your tables to ensure that they are transitioning from step to step on time.

- Provide some feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which table will be paired with your table before Step 3 begins.

- For the first round, assign one table as the presenting team and the other table as the customer.

- Have the presenting team present their solution to the customer team.

  - Have the customer team provide one objection for the presenting team to respond to.

  - The presentation, objections, and feedback should take no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have the table participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred target audience

\[insert your custom workshop content here . . . \]

## Preferred solution

_High-level architecture_

1. Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for creating a real-time data processing pipeline that can ingest, process, and write streaming data to a highly scalable managed PostgreSQL database on Azure. The stream processor needs to be resilient by keeping track of where it left off and prevent lost data. Include a solution for creating advanced visualizations that can be shared or embedded in external websites and mobile devices.

![High-level architecture.](../Media/outline-architecture.png 'High-level architecture')

The solution begins with multiple sources of clickstream data, each from a different tenant, flowing in through a Kafka streaming ingest managed service provided by Azure Event Hubs. This allows Wide World Importers to continue using their existing code to produce Kafka events. An Apache Spark cluster running on Azure Databricks processes and transforms the data in real time, using Structured Streaming. Azure Data Lake Storage is used to store the Structured Streaming checkpoint for resiliency to recover from errors and prevent lost data. Azure Key Vault is used to securely store secrets, such as the Event Hubs and PostgreSQL connection strings, and serves as a backing for Azure Databricks secret scopes. All event data is stored written to an Azure Databricks for PostgreSQL managed Hyperscale (Citus) database cluster, offering both scale-up and scale-out capability with features that simplify sharding and partitioning time series and multi-tenant data. Data is periodically written to rollup tables, using a background process that runs on a scheduled basis, to provide extremely fast querying of pre-aggregated data that does not interfere with incoming streams of late-arriving data. Websites and Power BI reports and dashboards use this data to provide rich reports that can be run at scale with minimum processing time. An on-premises data gateway cluster runs on several VMs to update the Power BI dashboards at regular intervals to match the pre-aggregation processes that write to the rollup tables.

_Scale_

1. How will you configure the managed PostgreSQL database so it can be scaled to meet demand. Think about options for scaling up and for scaling out.

   Azure Database for PostgreSQL is a fully managed service built on the open source PostgreSQL database engine. It provides high availability and performance with monitoring and alerting, enterprise-grade security and compliance, automatic backups, data secured at-rest and in-motion, and full compatibility with PostgreSQL extensions with little to no administration required.

   Hyperscale clusters are enabled by Citus, which is an extension of PostgreSQL that allows you to horizontally scale queries across multiple machines, using sharding and partitioning techniques. Because of this, the query engine parallelizes incoming SQL queries across these servers for faster response times on large datasets. Data sharding is transparently handled for you and allows your existing PostgreSQL applications to take advantage of the benefits distributed data storage and querying provides, with minimal changes. This means that Wide World Importer's multi-tenant, real-time operational analytics requirements will be met with little effort on their part.

   A Hyperscale server group (database cluster) consists of a coordinator node and several worker nodes. To scale up, you define the number of cores and storage you require per coordinator node and worker node. You can select up to 32 vCores with 8 GiB RAM per vCore and up to 2 TiB of storage with up to 3 IOPS / GiB per node. To scale out, you can define the number of worker nodes, between 2 and 20. If you require more than 20 worker nodes, you can submit a support request.

   You can define the compute and storage settings independently for your nodes, giving you flexibility by adding more of one or the other depending on your needs. The storage includes database files, temporary files, transaction logs, and the Postgres server logs. The total amount of storage you provision also defines the I/O capacity available to each worker and coordinator node.

2. WWI wants to be able to scale their database out, but they've already expressed concerns about how sharding tables to accomplish this adds a lot of complexity and maintenance overhead, as well as required code changes. How would you propose they shard tables that need to have data distributed amongst the nodes?

   The sharding logic is handled for you by the Hyperscale server group (enabled by Citus), allowing you to horizontally scale your database across multiple managed Postgres servers. This provides you with multi-tenancy because the data is sharded by the same Tenant ID. Because we are sharding on the same ID for our raw events table and rollup tables, our data stored in both types of table are automatically co-located for us by Citus. Furthermore, this means that aggregations can be performed locally without crossing network boundaries when we insert our events data into the rollup tables. The dashboard queries that execute against the rollup tables are always for a particular tenant. Hyperscale clusters allow us to parallelize our aggregations across shards, then perform a SELECT on a rollup for a particular customer from the dashboard, and have it automatically routed to the appropriate shard.

   Because the sharding is transparently handled for you, WWI will be able to start using this capability with very minor changes to their schema and mostly without impacting their existing applications.

3. How can the clickstream time series event data be partitioned into 5-minute increments to avoid creating large indexes?

   This event data table can be partitioned by a timestamp, such as `event_time`, for the time series data. The sample script below creates a partition every 5 minutes, using [pg_partman](https://www.citusdata.com/blog/2018/01/24/citus-and-pg-partman-creating-a-scalable-time-series-database-on-PostgreSQL/).

   ```sql
   CREATE TABLE events(
       event_id serial,
       event_time timestamptz default now(),
       tenant_id bigint,
       event_type text,
       country text,
       browser text,
       device_id bigint,
       session_id bigint
   )
   PARTITION BY RANGE (event_time);

   --Create 5-minutes partitions
   SELECT partman.create_parent('public.events', 'event_time', 'native', '5 minutes');
   UPDATE partman.part_config SET infinite_time_partitions = true;

   SELECT create_distributed_table('events','tenant_id');
   ```

   Partitioning is the key to high performance and being able to scale out across several database nodes. One of the keys to fast data loading is to avoid using large indexes. Traditionally, you would use block-range (BRIN) indexes to speed up range scans over roughly-sorted data. However, when you have unsorted data, BRIN indexes tend to perform poorly. Partitioning helps keep indexes small. It does this by dividing tables into partitions, avoiding fragmentation of data while maintaining smaller indexes.

_Multi-tenancy_

1. The clickstream event data is multi-tenant by nature. Each tenant is denoted by a Tenant ID which is related to the source of the clickstream feed. How can WWI shard the raw event table by tenant across multiple nodes in a way that causes little impact to existing applications and maintenance overhead?

   As mentioned earlier, WWI can shard their data based on Tenant ID. When you use the `create_distributed_table` function, you define the distributed table name (such as `events`) and a distribution column to define which tenant owns which rows (such as `tenant_id`):

   ```sql
   SELECT create_distributed_table('events','tenant_id');
   ```

   The hyperscale deployment handles the distribution, or sharding, for you across all of the cluster's nodes. As stated before, you do not need to have knowledge of how the data is distributed when you read from or write to these tables.

2. WWI has expressed a desire to create rollup tables that contain pre-aggregated data for efficient reporting. These rollup tables should also be sharded by Tenant ID. How can WWI shard these tables as well and what data type can be used to rapidly obtain distinct counts within a small margin of error in a highly scalable way across partitions?

   WWI will shard the rollup tables in the same way they shard the events table:

   ```sql
   SELECT create_distributed_table('rollup_events_5min','tenant_id');
   ```

   Another important thing to note about the rollup tables is that it is highly recommended to use HyperLogLog (HLL) data types to very rapidly obtain distinct counts for devices and sessions (device_distinct_count and session_distinct_count). HyperLogLog is a fixed-size data structure that is extremely fast at estimating distinct value counts with tunable precision. For example, in 1280 bytes HLL can estimate the count of tens of billions of distinct values with only a few percent error (source).

   You can see the `hll` data type in the schema definition below for a rollup table:

   ```sql
   CREATE TABLE rollup_events_5min (
       tenant_id bigint,
       event_type text,
       country text,
       browser text,
       minute timestamptz,
       event_count bigint,
       device_distinct_count hll,
       session_distinct_count hll,
       top_devices_1000 jsonb
   );
   CREATE UNIQUE INDEX rollup_events_5min_unique_idx ON rollup_events_5min(tenant_id,event_type,country,browser,minute);
   SELECT create_distributed_table('rollup_events_5min','tenant_id');
   ```

   It is very common to run SELECT COUNT(DISTINCT) on your database to update a dashboard with the number of unique items such as unique purchases of a particular item, unique users, unique page visits, etc. However, when you are using distributed systems, as Wide World Importers is in this situation, calculating unique counts is a difficult problem to solve. One reason for this is that there can be overlapping records across the workers. You could get around this by pulling all the data into a single machine and perform the count, but this does not scale well. Another option is to perform map/reduce functions, which scales, but are very slow to execute. The better option that provides scalability and speed is to use approximation algorithms to provide distinct count results within mathematically provable error bounds. This is why we are using HyperLogLog.

   If we were not using HLL, we would be limited to creating a large number of rollup tables. You would need rollup tables for various time periods, and rollup tables to calculate the distinct counts constrained by combinations of columns. For example, if you pre-aggregate over minutes, then you cannot answer queries asking for distinct counts over an hour. If you try and each minute's result to find hourly visits to a specific page, for example, the result will be unreliable because you are likely to have overlapping records within those different minutes. This problem is further complicated when you want to return a count of page visits filtered by time and unique page visit counts by user or a combination of the two. HLL allows us to use one or two rollup tables to answer all of these queries and more. This is because HLL overcomes the overlapping records problem by encoding the data in a way that allows summing up individual unique counts without re-counting overlapping records. When we write data to the HLL columns, we also hash it to ensure uniform distribution.

_Process data while generating roll-ups_

1. Rollup tables enable faster queries for reporting and exploration, but oftentimes require compute-heavy work to periodically run in the background to populate these tables. How would you schedule these aggregates to run on a periodic basis?

   WWI will create rollup aggregation functions that can be scheduled to run on a periodic basis using [pg_cron](https://github.com/citusdata/pg_cron).

   ```sql
   SELECT cron.schedule('*/5 * * * *', 'SELECT five_minutely_aggregation();');
   ```

   The cron notation makes it simple to schedule execution in a number of ways, such as minutely, hourly, daily, weekly, monthly, day of week, etc.

2. Within your rollup functions that perform the background aggregations, how would you implement incremental aggregations to handle late, incoming, data while keeping track of which time periods have been aggregated already?

   Rollups are an integral piece of this solution because they provide fast, indexed lookups of aggregates where compute-heavy work is performed periodically in the background. Because these rollups are compact, they can easily be consumed by various clients and kept over longer periods of time.

   When you look at the SQL scripts for the `five_minutely_aggregation` function below, you will notice that we are using incremental aggregation to support late, or incoming, data. This is accomplished by using `ON CONFLICT ... DO UPDATE` in the `INSERT` statement.

   ```sql
   CREATE OR REPLACE FUNCTION five_minutely_aggregation(OUT start_id bigint, OUT end_id bigint)
   RETURNS record
   LANGUAGE plpgsql
   AS $function$
   BEGIN
       /* determine which page views we can safely aggregate */
       SELECT window_start, window_end INTO start_id, end_id
       FROM incremental_rollup_window('rollup_events_5min');

       /* exit early if there are no new page views to aggregate */
       IF start_id > end_id THEN RETURN; END IF;

       /* aggregate the page views, merge results if the entry already exists */
       INSERT INTO rollup_events_5min
           SELECT customer_id,
                   event_type,
                   country,
                   browser,
                   date_trunc('seconds', (event_time - TIMESTAMP 'epoch') / 300) * 300 + TIMESTAMP 'epoch' AS minute,
                   count(*) as event_count,
                   hll_add_agg(hll_hash_bigint(device_id)) as device_distinct_count,
                   hll_add_agg(hll_hash_bigint(session_id)) as session_distinct_count,
                   topn_add_agg(device_id::text) top_devices_1000
           FROM events WHERE event_id BETWEEN start_id AND end_id
           GROUP BY customer_id,event_type,country,browser,minute
           ON CONFLICT (customer_id,event_type,country,browser,minute)
           DO UPDATE
           SET event_count=rollup_events_5min.event_count+excluded.event_count,
               device_distinct_count = hll_union(rollup_events_5min.device_distinct_count, excluded.device_distinct_count),
               session_distinct_count= hll_union(rollup_events_5min.session_distinct_count, excluded.session_distinct_count),
               top_devices_1000 = topn_union(rollup_events_5min.top_devices_1000, excluded.top_devices_1000);

   END;
   $function$;
   ```

   When executing aggregations, you have the choice between append-only or incremental aggregation. Append-only aggregation (insert) supports all aggregates, including exact distinct and percentiles, but are more difficult to use when handling late data. This is because you have to keep track of which time periods have been aggregated already, since you aggregate events for a particular time period and append them to the rollup table once all the data for that period are available. Incremental aggregation (upsert), on the other hand, easily supports processing late data. The side effect is that it cannot handle all aggregates. We work around this limitation by using highly accurate approximation through HyperLogLog (HLL) and `TopN`. As stated previously, WWI will be aggregating new events and upserting them to the rollup tables. They still need to be able to keep track of which events have already been aggregated.

   One way to keep track of which events have already been aggregated is to mark them as aggregated (`SET aggregated = true`). The problem with this approach is that it causes bloat and fragmentation. Another way would be to use a staging table to temporarily store events. This can cause catalog bloat and high overhead per batch, depending on how often your aggregation is run. The recommended approach is to [track the sequence number](https://www.citusdata.com/blog/2018/06/14/scalable-incremental-data-aggregation/). This means that each event has a monotonically increasing sequence number (`i`). We store sequence number `S` up to the point in which all events were aggregated. To aggregate, we pull a number from the sequence (`E`), briefly block writes to ensure there are no more in-flight transactions using sequence numbers <= `E` (`EXECUTE format('LOCK %s IN EXCLUSIVE MODE', table_to_lock)`), then incrementally aggregate all events with sequence numbers `S` < `i` <= `E`. Finally, we set `S` = `E` and repeat this process on each upsert. You can see exactly how we're doing this in the `incremental_rollup_window` function below. The `rollups` table keeps track of the sequence for us. The `five_minutely_aggregation` and `hourly_aggregation` functions call `incremental_rollup_window` to retrieve the range of page views that can be safely aggregated, using the start and end `event_id` values (`start_id` and `end_id`).

   ```sql
   CREATE TABLE rollups (
      name text primary key,
      event_table_name text not null,
      event_id_sequence_name text not null,
      last_aggregated_id bigint default 0
   );

   CREATE OR REPLACE FUNCTION incremental_rollup_window(rollup_name text, OUT window_start bigint, OUT window_end bigint)
   RETURNS record
   LANGUAGE plpgsql
   AS $function$
   DECLARE
      table_to_lock regclass;
   BEGIN
      /*
      * Perform aggregation from the last aggregated ID + 1 up to the last committed ID.
      * We do a SELECT .. FOR UPDATE on the row in the rollup table to prevent
      * aggregations from running concurrently.
      */
      SELECT event_table_name, last_aggregated_id+1, pg_sequence_last_value(event_id_sequence_name)
      INTO table_to_lock, window_start, window_end
      FROM rollups
      WHERE name = rollup_name FOR UPDATE;

      IF NOT FOUND THEN
          RAISE 'rollup ''%'' is not in the rollups table', rollup_name;
      END IF;

      IF window_end IS NULL THEN
          /* sequence was never used */
          window_end := 0;
          RETURN;
      END IF;

      /*
      * Play a little trick: We very briefly lock the table for writes in order to
      * wait for all pending writes to finish. That way, we are sure that there are
      * no more uncommitted writes with a identifier lower or equal to window_end.
      * By throwing an exception, we release the lock immediately after obtaining it
      * such that writes can resume.
      */
      BEGIN
          EXECUTE format('LOCK %s IN EXCLUSIVE MODE', table_to_lock);
          RAISE 'release table lock';
      EXCEPTION WHEN OTHERS THEN
      END;

      /*
      * Remember the end of the window to continue from there next time.
      */
      UPDATE rollups SET last_aggregated_id = window_end WHERE name = rollup_name;
   END;
   $function$;

   -- Entries for the rollup tables so that they are getting tracked in incremental rollup process.
   INSERT INTO rollups (name, event_table_name, event_id_sequence_name)
   VALUES ('rollup_events_5min', 'events','events_event_id_seq');
   ```

3. Incremental aggregations sacrifice the ability to handle all types of aggregates for ease of use when tracking what has already been aggregated when processing late data. What advanced aggregation options can you use to provide highly accurate approximation in this situation?

   Advanced aggregation is accomplished by using HyperLogLog (HLL) and `TopN`, as discussed earlier. For this topic, reference the `five_minutely_aggregation` function above. Also, please note that where you see the special `excluded` table in the query, it is used to reference values originally proposed for insertion. We are using `hll_has_bigint` to hash the HLL columns `device_id` and `session_id`. This hash function produces a uniformly distributed bit string. HLL does this by dividing values into streams and averaging the results. The `hll_add_agg` and `hll_union` are used to do incremental rollups. `TopN` keeps track of a set of counters in JSONB with the explicit goal of determining the top N (like top 10) items (or our "heavy hitters"). In our case, we're using it to return the top 1000 devices by `device_id`. Similar to HLL, we are using `topn_add_agg` and `topn_union` to do incremental rollups. The `topn_union` function merges `TopN` objects over time periods and dimensions.

_Resilient stream processing_

1. WWI is currently using a Kafka cluster to ingest a high volume of streaming data from various clickstream sources across their tenants. Is there a managed option in Azure that supports Kafka?

   Azure Event Hubs is a Big Data streaming platform that is capable of ingesting millions of events per second. It is fully managed and contains controls you can use to either scale it on demand or have it automatically scale to pre-defined thresholds. It provides an Apache Kafka endpoint you can enable, expanding its protocol options to HTTPS, AMQP, and Apache Kafka 1.0 and above. The Kafka endpoint would allow WWI to use their existing applications to write to the Event Hubs-backed Kafka service.

2. WWI sometimes encounters errors while stream processing, and has a difficult time recovering and continuing where they left off if the stream has to stop for any reason. What would you recommend for a resilient stream processing solution to reduce errors and prevent lost data?

   Apache Spark is the de facto standard in Big Data processing that is built to conduct batch, interactive, and real-time processing using the same core engine. This gives you the benefit of having a unified framework for processing these varying latencies (speeds) of Big Data, as well as being able to conduct machine learning, deep learning, graph processing, and advanced analytics in one place. Spark has become a leader in the data engineering and data science industry due to its ease of use, large ecosystem of tools and libraries, and for the fact that it runs batch and streaming workloads 100x faster than Hadoop on average. Azure Databricks provides one of the best options for running Spark on managed clusters in Azure.

   WWI can connect to the Kafka endpoint provided by Event Hubs, directly from a Databricks notebook, and use Spark Structured Streaming to process the data in real-time. Azure Key Vault can be used to secure secrets, such as the Kafka and PostgreSQL connection strings and be used as a backing store for a Databricks secret store. This will prevent any accidental leakage of these secrets.

   In Structured Streaming, if you enable checkpointing for a streaming query, then you can restart the query after a failure and the restarted query will continue where the failed one left off, while ensuring fault tolerance and data consistency guarantees. If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.

   The Databricks clusters can be configured to mount an Azure Data Lake Storage Gen2 account for fast-distributed storage of the streaming checkpoints, as well as any long-term storage needs when using Azure Databricks for Big Data analytics.

3. Could your chosen stream processing solution also provide the ability to conduct batch processing against large amounts of data while sharing much of the data processing and cleansing code, as well as code to write data to PostgreSQL?

   As stated in the previous answer, Apache Spark offers a unified platform for performing batch, interactive query, and real-time stream processing. Clusters are defined by selecting VM sizes and capabilities (compute or memory-optimized, GPU, etc.), as well as the number of worker nodes for scaling out Spark processing to handle demanding workloads. You can use Python, Scala, SQL, and R, along with a broad ecosystem of libraries you can use for data manipulation and advanced analytics.

   To connect to PostgreSQL from Azure Databricks, you can use the included `org.postgresql.Driver` driver and connect to it with a simple JDBC connection string that can be securely stored in Azure Key Vault and access from within a notebook or library.

_Advanced dashboards_

1. What would you recommend Wide World Importers use to create reports and dashboards with advanced visualizations, that can be created with an intuitive visual interface, easily shared with others, embedded in external websites or mobile devices?

   Power BI could be used to create reports and static or real-time dashboards from multiple on-premises and cloud-based services, such as Azure Database for PostgreSQL. WWI can use it to create advanced visualizations through a drag-and-drop interface with several out-of-the-box components, like maps, multiple charts, slicers, custom R and Python components, etc. They would start out by creating the report using Power BI Desktop to connect to the PostgreSQL database cluster, then optionally publish the report to the Power BI online service. From there, they can invite collaborators to modify the report, create a dashboard, modify for mobile form factors, and embed the report within external websites and mobile devices.

2. How can WWI refresh the report's data on a regular basis and provide redundancy in their synchronization process? Does this synchronization process require any inbound ports to be opened up on the computer or servers on which it runs?

   At this time, PostgreSQL data sources do not support DirectQuery or live updates in Power BI. However, the data source can be refreshed either manually or at regular intervals with an on-premises data gateway. The gateway can be installed on individual computers, but WWI would want to install them on servers or hosted VMs. When you add additional gateways, they act together as a highly available cluster. Behind the scenes, the gateway cloud service securely encrypts stores the data source credentials, and manages an Azure Service Bus instance to securely and resiliently secure data transfers between the cloud and gateways. This communication flows one way from the source gateways to the cloud. This way, it is not required to open any inbound traffic on the VMs or on-premises firewalls.

   WWI can perform multiple dataset refreshes daily to their PostgreSQL database cluster. Power BI limits datasets on shared capacity to eight daily refreshes. If the dataset resides on a Premium capacity, they can perform up to 48 refreshes per day.

## Checklist of preferred objection handling

1. Does Azure offer a managed PostgreSQL database that can handle our scale requirements?

2. We are worried about the re-engineering effort involved in sharding our database, from modifying the schema to updating our applications to account for the changes.

3. Is there a way to migrate to PostgreSQL on Azure with minimal downtime?

4. We've looked at several PostgreSQL-based data platforms for adding enhancements like distributed data and scalability, but we are concerned about our existing applications being compatible and having access to the latest versions of PostgreSQL.

## Customer quote (to be read back to the attendees at the end)

\[insert your custom workshop content here . . . \]
