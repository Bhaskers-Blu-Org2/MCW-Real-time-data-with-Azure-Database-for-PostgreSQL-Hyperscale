![Microsoft Cloud Workshop](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/master/Media/ms-cloud-workshop.png 'Microsoft Cloud Workshops')

<div class="MCWHeader1">
Managed open source databases on Azure
</div>

<div class="MCWHeader2">
Workshop outline
</div>

<div class="MCWHeader3">
June 2019
</div>

# Managed open source databases on Azure outline

Wide World Importers (WWI) is a traditional brick and mortar business with a long track record of success, generating profits through strong retail store sales of their unique offering of affordable products from around the world. They have a great training program for new employees, that focuses on connecting with their customers and providing great face-to-face customer service. This strong focus on customer relationships has helped set WWI apart from their competitors.

Over time, WWI modernized their business by expanding to online storefronts. During this expansion period, WWI experimented with various marketing tactics to drive online sales, from offering in-store shoppers special discounts online with promotional coupons after making a purchase, to running ad campaigns targeted toward customers based on demographics and shopping habits. These marketing campaigns proved successful, prompting WWI to invest more resources to these efforts and grow their marketing team.

Today, WWI has a host of online stores for various product offerings, from their traditional product catalogs offered by their physical storefronts, to specialized categories like automotive and consumer technology products. This expansion has made it more challenging to analyze user clickstream data, online ad performance, and other marketing campaigns at scale, and to provide insights to the marketing team in real-time.

Real-time marketing analysis is provided through interactive reports and dashboards on WWI's home-grown web platform, ReMarketable. This platform has served them well, but they are currently hindered by their inability to keep up with demand. ReMarketable's primary users are members of the marketing team, and the secondary users are shoppers on their various online platforms for whom website interaction behavior is being tracked. Other sources of data are fed from online ad data generated by ads run on social media platforms and email marketing campaigns. They use this type of data to evaluate ad effectiveness and customer reach, ultimately leading to sales conversions.

Their current challenges with ReMarketable are:

1. **Scale** - WWI is using a PostgreSQL database to store ReMarketable's data. Historical data is growing by over 2.9 GB rows of data per month. It is taking consistently longer to run complex queries. Queries that used to run in 3-5 seconds now take several minutes to complete. This is impacting their users' ability to evaluate up-to-date marketing and website use statistics. Instead of providing real-time reports for all users, they have to keep delaying report runs. They have scaled up their database, but this is becoming very expensive and they will soon hit a ceiling.
2. **Multi-tenancy** - The nature of their marketing and site usage data would benefit from multi-tenancy. Some storefronts generate considerably more data than others and have more marketing analysts that run reports on them than others. WWI believes sharding their database would help take the pressure off lower-volume data stores and also help them scale out. However, this will require re-engineering their database schema and client applications. In addition, sharding will require additional maintenance and increased complexity of aggregated views. These additional challenges and required resources are why they have not pursued this option yet.
3. **Process data while generating roll-ups** - Another byproduct of outgrowing their database is that WWI is having difficulty efficiently processing and ingesting streaming data, while at the same time generating pre-aggregated data for their dashboards. The Postgres engine is well-suited to handle multiple workloads simultaneously when the databases are properly configured and you are able to appropriately scale up or scale out to multiple nodes. WWI does needs help optimizing their database to handle these demanding workloads at scale. They have looked moving to a non-relational database to speed up queries, but that option added too much complexity to manage multiple databases, losing the ability to wrap their operations inside of transactions, re-architect their application, and migrate their historical data. In addition, they rely on Postgres' ability to create complicated ways of representing and indexing their data, which is impossible to do with a column store. Their need for high transaction volume and a real-time data set ruled out a lot of off-the-shelf data warehouses, where they would need to create a lambda architecture to handle both speeds of feeds.
4. **Resilient stream processing** - WWI is processing their streaming data through a web-based cluster that balances HTTP requests in round-robin fashion. When a node is processing the data and writing it to Postgres, subsequent requests are handled by available nodes. However, if processing fails for any reason, they risk losing that data and have no way to pick up where it left off. They have tried creating their own poison queue to reprocess these failed messages, but if the failed node is unable to add the data to the queue, then it is lost. The WWI technical team is aware of existing products and services that can help improve their stream processing and add resiliency, but they currently lack the skills and bandwidth to implement a solution for these complex scenarios. They are interested to see how Azure can help them rapidly create a solution for resilient stream processing and reduce their technical debt.
5. **Flat file processing & Advanced dashboards** - ~~WWI would like to start using public data sets, like the US census, to explore, visualize, and enrich their customer and market data with demographic information. currently they do not have the ability to perform data exploration at that scale when working with flat file data sources. They are looking for recommendations that will fit within their overall architecture.~~ In addition, they create canned reports that are displayed on their ReMarketable website. However, their developers spend a lot of time creating new reports, owing to advanced charts, graphs, and other visualizations that are usually included. They would like a way to more rapidly create reports and be able to display them on a dashboard that can be customized and show real-time updates.

## Target audience

- Database Administrator
- Data Engineer
- Data Scientist
- Database Developer
- Solution Architect

## Abstract

### Workshop

In this workshop, you will learn how to use advanced features of the managed PostgreSQL PaaS service on Azure to make your database more scalable and able to handle the rapid ingest of streaming data while simultaneously generating and serving pre-aggregated data for reports. You will design a resilient stream processing pipeline to ingest, process, and save real-time data to Postgres. Then, you will create complex reports containing advanced visualizations, using a drag-and-drop interface, and use them to build a customizable dashboard that can be easily shared with others.

At the end of this workshop, you will be better able to implement a highly scalable, managed open source database solution that can simultaneously handle real-time data and roll-up and serve data for advanced visualizations.

### Whiteboard design session _(this will go in the readme and in the WDS document)_

In this whiteboard design session, you will work with a group to design a solution for using advanced features of the managed PostgreSQL PaaS service on Azure to make your database more scalable and able to handle the rapid ingest of streaming data while simultaneously generating and serving pre-aggregated data for reports. You will provide guidance for designing a resilient stream processing pipeline to ingest, process, and save real-time data to Postgres. Then, you will provide guidance on how to create complex reports containing advanced visualizations, using a drag-and-drop interface, and use them to build a customizable dashboard that can be easily shared with others.

At the end of this whiteboard design session, you will be better able to design a highly scalable, managed open source database solution that can simultaneously handle real-time data and roll-up and serve data for advanced visualizations.

### Hands-on lab _(this will go in the readme and in the HOL document)_

In this hands-on lab, you will implement a proof-of-concept (PoC) for using advanced features of the managed PostgreSQL PaaS service on Azure. These features help make your database more scalable and able to handle the rapid ingest of streaming data while simultaneously generating and serving pre-aggregated data for reports. You will create a resilient stream processing pipeline to ingest, process, and save real-time data to Postgres. Then, you will create complex reports containing advanced visualizations, using a drag-and-drop interface, and use them to build a customizable dashboard that be easily shared with others.

At the end of this hands-on-lab, you will be better able to implement a highly scalable, managed open source database solution that can simultaneously handle real-time data and roll-up and serve data for advanced visualizations.

#### Outline: Hands-on lab exercises

- Before the hands-on lab (30 minutes)
  - Task 1: Set up Lab VM
  - Task 2: Deploy Azure Database for PostgreSQL
  - Task 3: Create Azure Databricks workspace
  - Task 4: Create Event Hubs namespace
  - Task 5: Create Databricks cluster
  - Task 6: Create a PostgreSQL server-level firewall rule
- Exercise 1: Connect to and set up your database
  - Task 1: Connect to the PostgreSQL database
  - Task 2: Create a schema
  - Task 3: Shard tables across nodes
- Exercise 2: Configure real-time data pipeline
  - Task 1: Add an event hub to your Event Hubs namespace
  - Task 2: Connect to event hub from Databricks notebook
  - Task 3: Use structured streaming to process real-time events
  - Task 4: Write data to PostgreSQL database
  - Task 5: Send sample real-time data
- Exercise 3: Explore flat files
  - Task 1: Create a new Python notebook
  - Task 2: Explore US Census data
  - Task 3: Aggregate and save results to PostgreSQL database
- Exercise 4: Rollup real-time data in Postgres
  - Task 1: Create function to rollup data
  - Task 2: Schedule rollups at regular intervals
- Exercise 5: Create advanced reports and dashboards
  - Task 1: Connect to your Postgres data from Power BI
  - Task 2: Create report
  - Task 3: Create dashboard with live update
- Exercise 7: Cleanup
  - Task 1: Delete resource group

## Azure services and related products

- Azure Database for PostgreSQL
- Azure Event Hubs with Kafka
- Azure Databricks
- Power BI
- Azure Cloud Shell
- pgAdmin

## Azure solutions

_This is an internal reference and will be updated by project PM._

## Related references

_This should be a list of and links to prereqs, architectural diagrams, supporting docs, or briefing decks related to the material._

- [MCW](https://github.com/Microsoft/MCW)
